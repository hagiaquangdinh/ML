import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from scipy.stats import zscore
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import mean_squared_error
import mlflow
import io
exp = mlflow.set_experiment("data_processing_experiment")
with mlflow.start_run(experiment_id=exp.experiment_id):
    def tien_xu_ly_du_lieu(updates_file=None):
        if updates_file is not None:
            df = pd.read_csv(updates_file)
        else:
            df = pd.read_csv("buoi2/data.txt")
        
        df = pd.read_csv("buoi2/data.txt")

        columns_to_drop = ["Cabin", "Ticket", "Name"]
        df.drop(columns=columns_to_drop, inplace=True)
        
        df['Age'].fillna(df['Age'].mean(), inplace=True)
        df['Fare'].fillna(df['Fare'].median(), inplace=True)
        df.dropna(subset=['Embarked'], inplace=True)

        df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})
        df['Embarked'] = df['Embarked'].map({'Q': 0, 'S': 1, 'C': 2})

        scaler = StandardScaler()
        df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])

        return df



    def test_train_size(actual_train_ratio, val_ratio_within_train, test_ratio):
        df = tien_xu_ly_du_lieu()
        X = df.drop(columns=['Survived'])
        y = df['Survived']
   
        # Chuy·ªÉn ƒë·ªïi t·ª∑ l·ªá ph·∫ßn trƒÉm th√†nh gi√° tr·ªã th·ª±c
        actual_train_size = actual_train_ratio / 100
        test_size = test_ratio / 100
        val_size = (val_ratio_within_train / 100) * actual_train_size  # Validation t·ª´ t·∫≠p Train
        
        # Chia t·∫≠p Train-Test tr∆∞·ªõc
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)

        # Ti·∫øp t·ª•c chia t·∫≠p Train th√†nh Train-Validation
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size / actual_train_size, stratify=y_train, random_state=42)

        # ƒê·∫£m b·∫£o r·∫±ng s·ªë l·∫ßn chia c·ªßa KFold h·ª£p l·ªá
        num_splits = max(2, int(1 / test_size))  # ƒê·∫£m b·∫£o n_splits >= 
        
        kf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)
        
        
        return X_train, X_val, X_test, y_train, y_val, y_test, kf, df

    
    def train_multiple_linear_regression(X_train, y_train, learning_rate=0.001, n_iterations=200):
        """Hu·∫•n luy·ªán h·ªìi quy tuy·∫øn t√≠nh b·ªôi b·∫±ng Gradient Descent."""
        
        # Chuy·ªÉn ƒë·ªïi X_train, y_train sang NumPy array ƒë·ªÉ tr√°nh l·ªói
        X_train = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train
        y_train = y_train.to_numpy().reshape(-1, 1) if isinstance(y_train, (pd.Series, pd.DataFrame)) else y_train.reshape(-1, 1)

        # Ki·ªÉm tra NaN ho·∫∑c Inf
        if np.isnan(X_train).any() or np.isnan(y_train).any():
            raise ValueError("D·ªØ li·ªáu ƒë·∫ßu v√†o ch·ª©a gi√° tr·ªã NaN!")
        if np.isinf(X_train).any() or np.isinf(y_train).any():
            raise ValueError("D·ªØ li·ªáu ƒë·∫ßu v√†o ch·ª©a gi√° tr·ªã v√¥ c√πng (Inf)!")

        # Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ tr√°nh tr√†n s·ªë
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)

        # L·∫•y s·ªë l∆∞·ª£ng m·∫´u (m) v√† s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n)
        m, n = X_train.shape
        #st.write(f"S·ªë l∆∞·ª£ng m·∫´u (m): {m}, S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n): {n}")

        # Th√™m c·ªôt bias (x0 = 1) v√†o X_train
        X_b = np.c_[np.ones((m, 1)), X_train]
        #st.write(f"K√≠ch th∆∞·ªõc ma tr·∫≠n X_b: {X_b.shape}")

        # Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n nh·ªè
        w = np.random.randn(X_b.shape[1], 1) * 0.01  
        #st.write(f"Tr·ªçng s·ªë ban ƒë·∫ßu: {w.flatten()}")

        # Gradient Descent
        for iteration in range(n_iterations):
            gradients = (2/m) * X_b.T.dot(X_b.dot(w) - y_train)

            # Ki·ªÉm tra xem gradients c√≥ NaN kh√¥ng
            # st.write(gradients)
            if np.isnan(gradients).any():
                raise ValueError("Gradient ch·ª©a gi√° tr·ªã NaN! H√£y ki·ªÉm tra l·∫°i d·ªØ li·ªáu ho·∫∑c learning rate.")

            w -= learning_rate * gradients

        #st.success("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")
        #st.write(f"Tr·ªçng s·ªë cu·ªëi c√πng: {w.flatten()}")
        return w



    # def train_polynomial_regression(X_train, y_train, degree=2, learning_rate=0.001, n_iterations=500):
    #     """Hu·∫•n luy·ªán h·ªìi quy ƒëa th·ª©c b·∫±ng Gradient Descent."""

    #     # Chuy·ªÉn d·ªØ li·ªáu sang NumPy array n·∫øu l√† pandas DataFrame/Series
    #     X_train = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train
    #     y_train = y_train.to_numpy().reshape(-1, 1) if isinstance(y_train, (pd.Series, pd.DataFrame)) else y_train.reshape(-1, 1)

    #     # T·∫°o ƒë·∫∑c tr∆∞ng ƒëa th·ª©c
    #     poly = PolynomialFeatures(degree=degree, include_bias=False)
    #     X_poly = poly.fit_transform(X_train)

    #     # Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ tr√°nh tr√†n s·ªë
    #     scaler = StandardScaler()
    #     X_poly = scaler.fit_transform(X_poly)

    #     # L·∫•y s·ªë l∆∞·ª£ng m·∫´u (m) v√† s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n)
    #     m, n = X_poly.shape
    #     st.write(f"S·ªë l∆∞·ª£ng m·∫´u (m): {m}, S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n): {n}")

    #     # Th√™m c·ªôt bias (x0 = 1)
    #     X_b = np.c_[np.ones((m, 1)), X_poly]
    #     st.write(f"K√≠ch th∆∞·ªõc ma tr·∫≠n X_b: {X_b.shape}")

    #     # Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n nh·ªè
    #     w = np.random.randn(X_b.shape[1], 1) * 0.01  
    #     st.write(f"Tr·ªçng s·ªë ban ƒë·∫ßu: {w.flatten()}")

    #     # Gradient Descent
    #     for iteration in range(n_iterations):
    #         gradients = (2/m) * X_b.T.dot(X_b.dot(w) - y_train)

    #         # Ki·ªÉm tra n·∫øu gradient c√≥ gi√° tr·ªã NaN
    #         if np.isnan(gradients).any():
    #             raise ValueError("Gradient ch·ª©a gi√° tr·ªã NaN! H√£y ki·ªÉm tra l·∫°i d·ªØ li·ªáu ho·∫∑c learning rate.")

    #         w -= learning_rate * gradients

    #     st.success("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")
    #     st.write(f"Tr·ªçng s·ªë cu·ªëi c√πng: {w.flatten()}")
        
    #     return w, poly, scaler
    
    def train_polynomial_regression(X_train, y_train, degree=2, learning_rate=0.001, n_iterations=500):
        """Hu·∫•n luy·ªán h·ªìi quy ƒëa th·ª©c **kh√¥ng c√≥ t∆∞∆°ng t√°c** b·∫±ng Gradient Descent."""

        # Chuy·ªÉn d·ªØ li·ªáu sang NumPy array n·∫øu l√† pandas DataFrame/Series
        X_train = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train
        y_train = y_train.to_numpy().reshape(-1, 1) if isinstance(y_train, (pd.Series, pd.DataFrame)) else y_train.reshape(-1, 1)

        # T·∫°o ƒë·∫∑c tr∆∞ng ƒëa th·ª©c **ch·ªâ th√™m b·∫≠c cao, kh√¥ng c√≥ t∆∞∆°ng t√°c**
        X_poly = np.hstack([X_train] + [X_train**d for d in range(2, degree + 1)])
        st.write(f"K√≠ch th∆∞·ªõc ma tr·∫≠n X_poly: {X_poly[1]}")
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ tr√°nh tr√†n s·ªë
        scaler = StandardScaler()
        X_poly = scaler.fit_transform(X_poly)

        # L·∫•y s·ªë l∆∞·ª£ng m·∫´u (m) v√† s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n)
        m, n = X_poly.shape
        print(f"S·ªë l∆∞·ª£ng m·∫´u (m): {m}, S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n): {n}")

        # Th√™m c·ªôt bias (x0 = 1)
        X_b = np.c_[np.ones((m, 1)), X_poly]
        print(f"K√≠ch th∆∞·ªõc ma tr·∫≠n X_b: {X_b.shape}")

        # Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n nh·ªè
        w = np.random.randn(X_b.shape[1], 1) * 0.01  
        print(f"Tr·ªçng s·ªë ban ƒë·∫ßu: {w.flatten()}")

        # Gradient Descent
        for iteration in range(n_iterations):
            gradients = (2/m) * X_b.T.dot(X_b.dot(w) - y_train)

            # Ki·ªÉm tra n·∫øu gradient c√≥ gi√° tr·ªã NaN
            if np.isnan(gradients).any():
                raise ValueError("Gradient ch·ª©a gi√° tr·ªã NaN! H√£y ki·ªÉm tra l·∫°i d·ªØ li·ªáu ho·∫∑c learning rate.")

            w -= learning_rate * gradients

        print("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")
        print(f"Tr·ªçng s·ªë cu·ªëi c√πng: {w.flatten()}")
        
        return w

    def chon_mo_hinh(model_type, X_train, X_val, X_test, y_train, y_val, y_test, kf):
        """Ch·ªçn m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi ho·∫∑c h·ªìi quy ƒëa th·ª©c."""
        degree = 2
        fold_mse = []  # Danh s√°ch MSE c·ªßa t·ª´ng fold
        scaler = StandardScaler()  # Chu·∫©n h√≥a d·ªØ li·ªáu cho h·ªìi quy ƒëa th·ª©c n·∫øu c·∫ßn

        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):
            X_train_fold, X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_train_fold, y_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]

            st.write(f"üöÄ Fold {fold + 1}: Train size = {len(X_train_fold)}, Validation size = {len(X_valid)}")


            if model_type == "linear":
                w= train_multiple_linear_regression(X_train_fold, y_train_fold)
    
                w = np.array(w).reshape(-1, 1)
                
                X_valid = X_valid.to_numpy()


                X_valid_b = np.c_[np.ones((len(X_valid), 1)), X_valid]  # Th√™m bias
                y_valid_pred = X_valid_b.dot(w)  # D·ª± ƒëo√°n
            elif model_type == "polynomial":
                
                X_train_fold = scaler.fit_transform(X_train_fold)
                 
                w = train_polynomial_regression(X_train_fold, y_train_fold, degree)
                
                w = np.array(w).reshape(-1, 1)
                
                X_valid_scaled = scaler.transform(X_valid.to_numpy())
                X_valid_poly = np.hstack([X_valid_scaled] + [X_valid_scaled**d for d in range(2, degree + 1)])
                X_valid_b = np.c_[np.ones((len(X_valid_poly), 1)), X_valid_poly]
                
                y_valid_pred = X_valid_b.dot(w)  # D·ª± ƒëo√°n
            else:
                raise ValueError("‚ö†Ô∏è Ch·ªçn 'linear' ho·∫∑c 'polynomial'!")

            mse = mean_squared_error(y_valid, y_valid_pred)
            fold_mse.append(mse)

            print(f"üìå Fold {fold + 1} - MSE: {mse:.4f}")

        # üî• Hu·∫•n luy·ªán l·∫°i tr√™n to√†n b·ªô t·∫≠p train
        if model_type == "linear":
            final_w = train_multiple_linear_regression(X_train, y_train)
            X_test_b = np.c_[np.ones((len(X_test), 1)), X_test]
            y_test_pred = X_test_b.dot(final_w)
        else:
            X_train_scaled = scaler.fit_transform(X_train)
            final_w = train_polynomial_regression(X_train_scaled, y_train, degree)

            X_test_scaled = scaler.transform(X_test.to_numpy())
            X_test_poly = np.hstack([X_test_scaled] + [X_test_scaled**d for d in range(2, degree + 1)])
            X_test_b = np.c_[np.ones((len(X_test_poly), 1)), X_test_poly]

            y_test_pred = X_test_b.dot(final_w)

        # üìå ƒê√°nh gi√° tr√™n t·∫≠p test
        test_mse = mean_squared_error(y_test, y_test_pred)
        avg_mse = np.mean(fold_mse)  # Trung b√¨nh MSE qua c√°c folds

        st.success(f"MSE trung b√¨nh qua c√°c folds: {avg_mse:.4f}")
        st.success(f"MSE tr√™n t·∫≠p test: {test_mse:.4f}")

        return final_w, avg_mse, scaler

    def bt_buoi3():
        uploaded_file = "buoi2/data.txt"
        try:
            df = pd.read_csv(uploaded_file, delimiter=",")
        except FileNotFoundError:
            st.error("‚ùå Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")
            st.stop()
        st.title("üîç Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")
        
        st.subheader("üìå 10 d√≤ng ƒë·∫ßu c·ªßa d·ªØ li·ªáu g·ªëc")
        st.write(df.head(10))
        
        st.subheader("üö® Ki·ªÉm tra l·ªói d·ªØ li·ªáu")

                    # Ki·ªÉm tra gi√° tr·ªã thi·∫øu
        missing_values = df.isnull().sum()

                    # Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p
        duplicate_count = df.duplicated().sum()

                    
                    
                    # Ki·ªÉm tra gi√° tr·ªã qu√° l·ªõn (outlier) b·∫±ng Z-score
        outlier_count = {
            col: (abs(zscore(df[col], nan_policy='omit')) > 3).sum()
            for col in df.select_dtypes(include=['number']).columns
        }

                    # T·∫°o b√°o c√°o l·ªói
        error_report = pd.DataFrame({
            'C·ªôt': df.columns,
            'Gi√° tr·ªã thi·∫øu': missing_values,
            'Outlier': [outlier_count.get(col, 0) for col in df.columns]
        })

                    # Hi·ªÉn th·ªã b√°o c√°o l·ªó
        st.table(error_report)

                    # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng d·ªØ li·ªáu tr√πng l·∫∑p
        st.write(f"üîÅ **S·ªë l∆∞·ª£ng d√≤ng b·ªã tr√πng l·∫∑p:** {duplicate_count}")         
        
        st.title("üîç Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu(Th·ª±c h√†nh ·ªü app ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu)")

        # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
        st.subheader("1Ô∏è‚É£ Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng quan tr·ªçng")
        st.write("""
        M·ªôt s·ªë c·ªôt trong d·ªØ li·ªáu c√≥ th·ªÉ kh√¥ng ƒë√≥ng g√≥p nhi·ªÅu v√†o k·∫øt qu·∫£ d·ª± ƒëo√°n ho·∫∑c ch·ª©a qu√° nhi·ªÅu gi√° tr·ªã thi·∫øu. Vi·ªác lo·∫°i b·ªè c√°c c·ªôt n√†y gi√∫p gi·∫£m ƒë·ªô ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh v√† c·∫£i thi·ªán hi·ªáu su·∫•t.
        """)

        # X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
        st.subheader("2Ô∏è‚É£ X·ª≠ l√Ω gi√° tr·ªã thi·∫øu")
        st.write("""
        D·ªØ li·ªáu th·ª±c t·∫ø th∆∞·ªùng ch·ª©a c√°c gi√° tr·ªã b·ªã thi·∫øu. Ta c·∫ßn l·ª±a ch·ªçn ph∆∞∆°ng ph√°p th√≠ch h·ª£p nh∆∞ ƒëi·ªÅn gi√° tr·ªã trung b√¨nh, lo·∫°i b·ªè h√†ng ho·∫∑c s·ª≠ d·ª•ng m√¥ h√¨nh d·ª± ƒëo√°n ƒë·ªÉ x·ª≠ l√Ω ch√∫ng nh·∫±m tr√°nh ·∫£nh h∆∞·ªüng ƒë·∫øn m√¥ h√¨nh.
        """)

        # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
        st.subheader("3Ô∏è‚É£ Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu")
        st.write("""
        M·ªôt s·ªë c·ªôt trong d·ªØ li·ªáu c√≥ th·ªÉ ch·ª©a gi√° tr·ªã d·∫°ng ch·ªØ (danh m·ª•c). ƒê·ªÉ m√¥ h√¨nh c√≥ th·ªÉ x·ª≠ l√Ω, ta c·∫ßn chuy·ªÉn ƒë·ªïi ch√∫ng th√†nh d·∫°ng s·ªë b·∫±ng c√°c ph∆∞∆°ng ph√°p nh∆∞ one-hot encoding ho·∫∑c label encoding.
        """)

        # Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë
        st.subheader("4Ô∏è‚É£ Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë")
        st.write("""
        C√°c gi√° tr·ªã s·ªë trong t·∫≠p d·ªØ li·ªáu c√≥ th·ªÉ c√≥ ph·∫°m vi r·∫•t kh√°c nhau, ƒëi·ªÅu n√†y c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë·ªô h·ªôi t·ª• c·ªßa m√¥ h√¨nh. Ta c·∫ßn chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng c√≥ c√πng tr·ªçng s·ªë khi hu·∫•n luy·ªán m√¥ h√¨nh.
        """)

        # Chia d·ªØ li·ªáu th√†nh t·∫≠p Train, Validation, v√† Test
        st.subheader("5Ô∏è‚É£ Chia d·ªØ li·ªáu th√†nh t·∫≠p Train, Validation, v√† Test")
        st.write("""
        ƒê·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët tr√™n d·ªØ li·ªáu th·ª±c t·∫ø, ta chia t·∫≠p d·ªØ li·ªáu th√†nh ba ph·∫ßn:
        - **Train**: D√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.
        - **Validation**: D√πng ƒë·ªÉ ƒëi·ªÅu ch·ªânh tham s·ªë m√¥ h√¨nh nh·∫±m t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t.
        - **Test**: D√πng ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t cu·ªëi c√πng c·ªßa m√¥ h√¨nh tr√™n d·ªØ li·ªáu ch∆∞a t·ª´ng th·∫•y.
        """)
        
        
        
        st.title("L·ª±a ch·ªçn thu·∫≠t to√°n h·ªçc m√°y: Multiple vs. Polynomial Regression")

        # Gi·ªõi thi·ªáu
        st.write("## 1. Multiple Linear Regression")
        st.write("""
        H·ªìi quy tuy·∫øn t√≠nh b·ªôi l√† m·ªôt thu·∫≠t to√°n h·ªçc m√°y c√≥ gi√°m s√°t, m√¥ t·∫£ m·ªëi quan h·ªá gi·ªØa m·ªôt bi·∫øn ph·ª• thu·ªôc (output) v√† nhi·ªÅu bi·∫øn ƒë·ªôc l·∫≠p (input) th√¥ng qua m·ªôt h√†m tuy·∫øn t√≠nh.
        V√≠ d·ª• d·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n di·ªán t√≠ch, s·ªë ph√≤ng, v·ªã tr√≠, ... 
        
        C√¥ng th·ª©c t·ªïng qu√°t c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi:
        """)
        st.image("buoi3/img1.png", caption="Multiple Linear Regression ƒë∆°n", use_container_width =True)
        st.latex(r"""
        y = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n
        """)

        
    
    

        # Gi·ªõi thi·ªáu Polynomial Regression
        st.write("## 2. Polynomial Regression")

        st.write("Polynomial Regression m·ªü r·ªông m√¥ h√¨nh tuy·∫øn t√≠nh b·∫±ng c√°ch th√™m c√°c b·∫≠c cao h∆°n c·ªßa bi·∫øn ƒë·∫ßu v√†o.")
        
        st.image("buoi3/img3.png", caption="Polynomial Regression ", use_container_width =True)
        st.write("""
        C√¥ng th·ª©c t·ªïng qu√°t c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi:
        """)
        st.latex(r"""
        y = w_0 + w_1x + w_2x^2 + w_3x^3 + \dots + w_nx^n
        """)

        
        st.write("""
        ### H√†m m·∫•t m√°t (Loss Function) c·ªßa Linear Regression
        H√†m m·∫•t m√°t ph·ªï bi·∫øn nh·∫•t l√† **Mean Squared Error (MSE)**:
        """)
        st.latex(r"""
        MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        """)

        st.markdown(r"""
        Trong ƒë√≥:
        - $n$: S·ªë l∆∞·ª£ng ƒëi·ªÉm d·ªØ li·ªáu.
        - $y_i$: Gi√° tr·ªã th·ª±c t·∫ø c·ªßa bi·∫øn ph·ª• thu·ªôc.
        - $\hat{y}_i$: Gi√° tr·ªã d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh.
        """)

        st.markdown(r"""
        M·ª•c ti√™u c·ªßa h·ªìi quy tuy·∫øn t√≠nh b·ªôi l√† t√¨m c√°c h·ªá s·ªë tr·ªçng s·ªë $w_0, w_1, w_2, ..., w_n$ sao cho gi√° tr·ªã MSE nh·ªè nh·∫•t.

        ### Thu·∫≠t to√°n Gradient Descent
        1. Kh·ªüi t·∫°o c√°c tr·ªçng s·ªë $w_0, w_1, w_2, ..., w_n$ v·ªõi gi√° tr·ªã b·∫•t k·ª≥.
        2. T√≠nh gradient c·ªßa MSE ƒë·ªëi v·ªõi t·ª´ng tr·ªçng s·ªë.
        3. C·∫≠p nh·∫≠t tr·ªçng s·ªë theo quy t·∫Øc c·ªßa thu·∫≠t to√°n Gradient Descent.

        ### ƒê√°nh gi√° m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi
        - **H·ªá s·ªë t∆∞∆°ng quan (R)**: ƒê√°nh gi√° m·ª©c ƒë·ªô t∆∞∆°ng quan gi·ªØa gi√° tr·ªã th·ª±c t·∫ø v√† gi√° tr·ªã d·ª± ƒëo√°n.
        - **H·ªá s·ªë x√°c ƒë·ªãnh (R¬≤)**: ƒêo l∆∞·ªùng ph·∫ßn trƒÉm bi·∫øn ƒë·ªông c·ªßa bi·∫øn ph·ª• thu·ªôc c√≥ th·ªÉ gi·∫£i th√≠ch b·ªüi c√°c bi·∫øn ƒë·ªôc l·∫≠p:
        """)
        st.latex(r"""
        R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
        """)

        st.write("""
        - **Adjusted R¬≤**: ƒêi·ªÅu ch·ªânh cho s·ªë l∆∞·ª£ng bi·∫øn ƒë·ªôc l·∫≠p, gi√∫p tr√°nh overfitting:
        """)
        st.latex(r"""
        R^2_{adj} = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - k - 1} \right)
        """)

        st.markdown(r"""
        Trong ƒë√≥:
        - $n$: S·ªë l∆∞·ª£ng quan s√°t.
        - $k$: S·ªë l∆∞·ª£ng bi·∫øn ƒë·ªôc l·∫≠p.
        - $\bar{y}$: Gi√° tr·ªã trung b√¨nh c·ªßa bi·∫øn ph·ª• thu·ªôc.
        """)

        st.write("""
        
        - **Sai s·ªë chu·∫©n (SE)**: ƒê√°nh gi√° m·ª©c ƒë·ªô ph√¢n t√°n c·ªßa sai s·ªë d·ª± ƒëo√°n quanh gi√° tr·ªã th·ª±c t·∫ø:
        """)
        st.latex(r"""
        SE = \sqrt{\frac{\sum (y_i - \hat{y}_i)^2}{n - k - 1}}
        """)

        st.write("""
        C√°c ch·ªâ s·ªë n√†y gi√∫p ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c v√† kh·∫£ nƒÉng kh√°i qu√°t h√≥a c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi.
        """)
        # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
        st.write("## 3. Minh h·ªça tr·ª±c quan")

        # T·∫°o d·ªØ li·ªáu m·∫´u
        np.random.seed(0)
        x = np.sort(5 * np.random.rand(20, 1), axis=0)
        y = 2 * x**2 - 3 * x + np.random.randn(20, 1) * 2

        # H·ªìi quy tuy·∫øn t√≠nh
        lin_reg = LinearRegression()
        lin_reg.fit(x, y)
        y_pred_linear = lin_reg.predict(x)

        # H·ªìi quy b·∫≠c hai
        poly_features = PolynomialFeatures(degree=2)
        x_poly = poly_features.fit_transform(x)
        poly_reg = LinearRegression()
        poly_reg.fit(x_poly, y)
        y_pred_poly = poly_reg.predict(x_poly)

        # V·∫Ω bi·ªÉu ƒë·ªì
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.scatter(x, y, color='blue', label='D·ªØ li·ªáu th·ª±c t·∫ø')
        ax.plot(x, y_pred_linear, color='red', label='Multiple Linear Regression')
        ax.plot(x, y_pred_poly, color='green', label='Polynomial Regression (b·∫≠c 2)')
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.legend()
        st.pyplot(fig)
        
        
        
        uploaded_file = st.file_uploader("üìÇ Ch·ªçn file d·ªØ li·ªáu (.csv ho·∫∑c .txt)", type=["csv", "txt"])
    # N·∫øu c√≥ file upload, s·ª≠ d·ª•ng n√≥, n·∫øu kh√¥ng d√πng file m·∫∑c ƒë·ªãnh
        df = tien_xu_ly_du_lieu(uploaded_file)
        st.write(df.head(10))
    

        
        train_ratio = st.slider("Ch·ªçn t·ª∑ l·ªá Train (%)", min_value=50, max_value=90, value=70, step=1)
        test_ratio = 100 - train_ratio  # Test t·ª± ƒë·ªông t√≠nh to√°n

        # Thanh k√©o ch·ªçn t·ª∑ l·ªá Validation tr√™n Train
        val_ratio_within_train = st.slider("Ch·ªçn t·ª∑ l·ªá Validation trong Train (%)", min_value=0, max_value=50, value=30, step=1)

        # T√≠nh to√°n l·∫°i t·ª∑ l·ªá Validation tr√™n to√†n b·ªô dataset
        val_ratio = (val_ratio_within_train / 100) * train_ratio
        actual_train_ratio = train_ratio - val_ratio

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        st.write(f"T·ª∑ l·ªá d·ªØ li·ªáu: Train = {actual_train_ratio:.1f}%, Validation = {val_ratio:.1f}%, Test = {test_ratio:.1f}%")

        X_train, X_val, X_test, y_train, y_val, y_test, kf, df  =test_train_size(actual_train_ratio, val_ratio_within_train,test_ratio)


        # Ch·ªçn m√¥ h√¨nh    
        model_type = st.radio("Ch·ªçn lo·∫°i m√¥ h√¨nh:", ["Multiple Linear Regression", "Polynomial Regression"])




        # Khi nh·∫•n n√∫t s·∫Ω hu·∫•n luy·ªán m√¥ h√¨nh
        if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh"):
        # X√°c ƒë·ªãnh model_type ph√π h·ª£p
            #mlflow.log_param("model_type", model_type)
        
            model_type_value = "linear" if model_type == "Multiple Linear Regression" else "polynomial"

            # G·ªçi h√†m v·ªõi ƒë√∫ng th·ª© t·ª± tham s·ªë
            final_w, avg_mse, poly, scaler= chon_mo_hinh(model_type_value, X_train, X_val, X_test, y_train, y_val, y_test, kf)


    
if __name__ == "__main__":
    bt_buoi3()