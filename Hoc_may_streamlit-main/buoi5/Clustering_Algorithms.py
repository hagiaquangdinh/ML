import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Táº£i dá»¯ liá»‡u MNIST tá»« OpenM




def data():
    st.header("MNIST Dataset")
    st.write("""
      **MNIST** lÃ  má»™t trong nhá»¯ng bá»™ dá»¯ liá»‡u ná»•i tiáº¿ng vÃ  phá»• biáº¿n nháº¥t trong cá»™ng Ä‘á»“ng há»c mÃ¡y, 
      Ä‘áº·c biá»‡t lÃ  trong cÃ¡c nghiÃªn cá»©u vá» nháº­n diá»‡n máº«u vÃ  phÃ¢n loáº¡i hÃ¬nh áº£nh.
  
      - Bá»™ dá»¯ liá»‡u bao gá»“m tá»•ng cá»™ng **70.000 áº£nh chá»¯ sá»‘ viáº¿t tay** tá»« **0** Ä‘áº¿n **9**, 
        má»—i áº£nh cÃ³ kÃ­ch thÆ°á»›c **28 x 28 pixel**.
      - Chia thÃ nh:
        - **Training set**: 60.000 áº£nh Ä‘á»ƒ huáº¥n luyá»‡n.
        - **Test set**: 10.000 áº£nh Ä‘á»ƒ kiá»ƒm tra.
      - Má»—i hÃ¬nh áº£nh lÃ  má»™t chá»¯ sá»‘ viáº¿t tay, Ä‘Æ°á»£c chuáº©n hÃ³a vÃ  chuyá»ƒn thÃ nh dáº¡ng grayscale (Ä‘en tráº¯ng).
  
      Dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh nháº­n diá»‡n chá»¯ sá»‘.
      """)

    st.subheader("Má»™t sá»‘ hÃ¬nh áº£nh tá»« MNIST Dataset")
    st.image("buoi4/img3.png", caption="Má»™t sá»‘ hÃ¬nh áº£nh tá»« MNIST Dataset", use_container_width=True)

    st.subheader("á»¨ng dá»¥ng thá»±c táº¿ cá»§a MNIST")
    st.write("""
      Bá»™ dá»¯ liá»‡u MNIST Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng trong nhiá»u á»©ng dá»¥ng nháº­n dáº¡ng chá»¯ sá»‘ viáº¿t tay, cháº³ng háº¡n nhÆ°:
      - Nháº­n diá»‡n sá»‘ trÃªn cÃ¡c hoÃ¡ Ä‘Æ¡n thanh toÃ¡n, biÃªn lai cá»­a hÃ ng.
      - Xá»­ lÃ½ chá»¯ sá»‘ trÃªn cÃ¡c bÆ°u kiá»‡n gá»­i qua bÆ°u Ä‘iá»‡n.
      - á»¨ng dá»¥ng trong cÃ¡c há»‡ thá»‘ng nháº­n diá»‡n tÃ i liá»‡u tá»± Ä‘á»™ng.
    """)

    st.subheader("VÃ­ dá»¥ vá» cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y vá»›i MNIST")
    st.write("""
      CÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y phá»• biáº¿n Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i bá»™ dá»¯ liá»‡u MNIST bao gá»“m:
      - **Logistic Regression**
      - **Decision Trees**
      - **K-Nearest Neighbors (KNN)**
      - **Support Vector Machines (SVM)**
      - **Convolutional Neural Networks (CNNs)**
    """)

    # st.subheader("Káº¿t quáº£ cá»§a má»™t sá»‘ mÃ´ hÃ¬nh trÃªn MNIST ")
    # st.write("""
    #   Äá»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y vá»›i MNIST, ngÆ°á»i ta thÆ°á»ng sá»­ dá»¥ng Ä‘á»™ chÃ­nh xÃ¡c (accuracy) trÃªn táº­p test:
      
    #   - **Decision Tree**: 0.8574
    #   - **SVM (Linear)**: 0.9253
    #   - **SVM (poly)**: 0.9774
    #   - **SVM (sigmoid)**: 0.7656
    #   - **SVM (rbf)**: 0.9823
      
      
      
    # """)

def ly_thuyet_K_means():
    st.title("ğŸ“Œ K-Means Clustering")

    # ğŸ”¹ Giá»›i thiá»‡u vá» K-Means
    st.markdown(r"""
        
        **K-Means** lÃ  má»™t thuáº­t toÃ¡n **phÃ¢n cá»¥m khÃ´ng giÃ¡m sÃ¡t** phá»• biáº¿n, giÃºp chia táº­p dá»¯ liá»‡u thÃ nh **K cá»¥m** sao cho cÃ¡c Ä‘iá»ƒm trong cÃ¹ng má»™t cá»¥m cÃ³ Ä‘áº·c trÆ°ng tÆ°Æ¡ng Ä‘á»“ng nháº¥t.  

        ---

        ### ğŸ”¹ **Ã tÆ°á»Ÿng chÃ­nh cá»§a K-Means**
        1ï¸âƒ£ **Khá»Ÿi táº¡o \( K \) tÃ¢m cá»¥m (centroids)** ngáº«u nhiÃªn tá»« táº­p dá»¯ liá»‡u.  
        2ï¸âƒ£ **GÃ¡n má»—i Ä‘iá»ƒm dá»¯ liá»‡u vÃ o cá»¥m cÃ³ tÃ¢m gáº§n nháº¥t**, sá»­ dá»¥ng khoáº£ng cÃ¡ch Euclidean:  
        """)

    st.latex(r"""
        d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}
        """)

    st.markdown(r"""
        3ï¸âƒ£ **Cáº­p nháº­t láº¡i tÃ¢m cá»¥m** báº±ng cÃ¡ch tÃ­nh trung bÃ¬nh cá»§a cÃ¡c Ä‘iá»ƒm trong cá»¥m:  
        """)

    st.latex(r"""
        \mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i
        """)

    st.markdown(r"""
        4ï¸âƒ£ **Láº·p láº¡i quÃ¡ trÃ¬nh trÃªn** cho Ä‘áº¿n khi khÃ´ng cÃ³ sá»± thay Ä‘á»•i hoáº·c Ä‘áº¡t Ä‘áº¿n sá»‘ vÃ²ng láº·p tá»‘i Ä‘a.  

        ---

        ### ğŸ”¢ **CÃ´ng thá»©c tá»‘i Æ°u hÃ³a trong K-Means**
        K-Means tÃ¬m cÃ¡ch tá»‘i thiá»ƒu hÃ³a tá»•ng bÃ¬nh phÆ°Æ¡ng khoáº£ng cÃ¡ch tá»« má»—i Ä‘iá»ƒm Ä‘áº¿n tÃ¢m cá»¥m cá»§a nÃ³:  
        """)

    st.latex(r"""
        J = \sum_{k=1}^{K} \sum_{x_i \in C_k} ||x_i - \mu_k||^2
        """)

    st.markdown(r"""
        Trong Ä‘Ã³:  
        - **$$ J $$**: HÃ m máº¥t mÃ¡t (tá»•ng bÃ¬nh phÆ°Æ¡ng khoáº£ng cÃ¡ch).  
        - **$$ x_i $$**: Äiá»ƒm dá»¯ liá»‡u thá»© $$ i $$.  
        - **$$ \mu_k $$**: TÃ¢m cá»¥m thá»© $$ k $$.  
        - **$$ C_k $$**: Táº­p cÃ¡c Ä‘iá»ƒm thuá»™c cá»¥m $$ k $$.  

        ---

        ### âœ… **Æ¯u Ä‘iá»ƒm & âŒ NhÆ°á»£c Ä‘iá»ƒm**
        âœ… **Æ¯u Ä‘iá»ƒm:**  
        - ÄÆ¡n giáº£n, dá»… hiá»ƒu, tá»‘c Ä‘á»™ nhanh.  
        - Hiá»‡u quáº£ trÃªn táº­p dá»¯ liá»‡u lá»›n.  
        - Dá»… triá»ƒn khai vÃ  má»Ÿ rá»™ng.  

        âŒ **NhÆ°á»£c Ä‘iá»ƒm:**  
        - Cáº§n xÃ¡c Ä‘á»‹nh sá»‘ cá»¥m \( K \) trÆ°á»›c.  
        - Nháº¡y cáº£m vá»›i giÃ¡ trá»‹ ngoáº¡i lai (**outliers**).  
        - Káº¿t quáº£ phá»¥ thuá»™c vÃ o cÃ¡ch khá»Ÿi táº¡o ban Ä‘áº§u cá»§a cÃ¡c tÃ¢m cá»¥m.  

        ---

        ### ğŸ” **Má»™t sá»‘ cáº£i tiáº¿n cá»§a K-Means**
        - **K-Means++**: Cáº£i thiá»‡n cÃ¡ch chá»n tÃ¢m cá»¥m ban Ä‘áº§u Ä‘á»ƒ giáº£m thiá»ƒu há»™i tá»¥ vÃ o cá»±c tiá»ƒu cá»¥c bá»™.  
        - **Mini-batch K-Means**: Sá»­ dá»¥ng táº­p máº«u nhá» Ä‘á»ƒ cáº­p nháº­t tÃ¢m cá»¥m, giÃºp tÄƒng tá»‘c Ä‘á»™ trÃªn dá»¯ liá»‡u lá»›n.  
        - **K-Medoids**: Thay vÃ¬ trung bÃ¬nh, sá»­ dá»¥ng Ä‘iá»ƒm thá»±c táº¿ lÃ m tÃ¢m cá»¥m Ä‘á»ƒ giáº£m áº£nh hÆ°á»Ÿng cá»§a outliers.  

        ğŸ“Œ **á»¨ng dá»¥ng cá»§a K-Means:** PhÃ¢n tÃ­ch khÃ¡ch hÃ ng, nháº­n diá»‡n máº«u, nÃ©n áº£nh, phÃ¢n cá»¥m vÄƒn báº£n, v.v.  
        """)



    # ğŸ”¹ Äá»‹nh nghÄ©a hÃ m tÃ­nh toÃ¡n
    def euclidean_distance(a, b):
        return np.linalg.norm(a - b, axis=1)

    def generate_data(n_samples, n_clusters):
        np.random.seed(42)
        X = []
        cluster_std = 1.0  # Äá»™ rá»i ráº¡c cá»‘ Ä‘á»‹nh
        centers = np.random.uniform(-10, 10, size=(n_clusters, 2))
        for c in centers:
            X.append(c + np.random.randn(n_samples // n_clusters, 2) * cluster_std)
        return np.vstack(X)

    def initialize_centroids(X, k):
        return X[np.random.choice(X.shape[0], k, replace=False)]

    def assign_clusters(X, centroids):
        return np.array([np.argmin(euclidean_distance(x, centroids)) for x in X])

    def update_centroids(X, labels, k):
        return np.array([X[labels == i].mean(axis=0) if len(X[labels == i]) > 0 else np.random.uniform(-10, 10, 2) for i in range(k)])

    # Giao diá»‡n Streamlit
    st.title("ğŸ¯ Minh há»a thuáº­t toÃ¡n K-Means tá»«ng bÆ°á»›c")

    num_samples_kmeans = st.slider("Sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u", 50, 500, 200, step=10)
    cluster_kmeans = st.slider("Sá»‘ cá»¥m (K)", 2, 10, 3)

    # Kiá»ƒm tra vÃ  cáº­p nháº­t dá»¯ liá»‡u khi tham sá»‘ thay Ä‘á»•i
    if "data_params" not in st.session_state or st.session_state.data_params != (num_samples_kmeans, cluster_kmeans):
        st.session_state.data_params = (num_samples_kmeans, cluster_kmeans)
        st.session_state.X = generate_data(num_samples_kmeans, cluster_kmeans)
        st.session_state.centroids = initialize_centroids(st.session_state.X, cluster_kmeans)
        st.session_state.iteration = 0
        st.session_state.labels = assign_clusters(st.session_state.X, st.session_state.centroids)

    X = st.session_state.X

    if st.button("ğŸ”„ Reset"):
        st.session_state.X = generate_data(num_samples_kmeans, cluster_kmeans)
        st.session_state.centroids = initialize_centroids(st.session_state.X, cluster_kmeans)
        st.session_state.iteration = 0
        st.session_state.labels = assign_clusters(st.session_state.X, st.session_state.centroids)

    if st.button("ğŸ”„ Cáº­p nháº­t vá»‹ trÃ­ tÃ¢m cá»¥m"):
        st.session_state.labels = assign_clusters(X, st.session_state.centroids)
        new_centroids = update_centroids(X, st.session_state.labels, cluster_kmeans)

        # Kiá»ƒm tra há»™i tá»¥ vá»›i sai sá»‘ nhá»
        if np.allclose(new_centroids, st.session_state.centroids, atol=1e-3):
            st.warning("âš ï¸ TÃ¢m cá»¥m khÃ´ng thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ, thuáº­t toÃ¡n Ä‘Ã£ há»™i tá»¥!")
        else:
            st.session_state.centroids = new_centroids
            st.session_state.iteration += 1

    # ğŸ”¥ ThÃªm thanh tráº¡ng thÃ¡i hiá»ƒn thá»‹ tiáº¿n trÃ¬nh
    st.status(f"Láº§n cáº­p nháº­t: {st.session_state.iteration} - Äang phÃ¢n cá»¥m...", state="running")
    st.markdown("### ğŸ“Œ Tá»a Ä‘á»™ tÃ¢m cá»¥m hiá»‡n táº¡i:")
    num_centroids = st.session_state.centroids.shape[0]
    centroid_df = pd.DataFrame(st.session_state.centroids, columns=["X", "Y"])
    centroid_df.index = [f"TÃ¢m cá»¥m {i}" for i in range(num_centroids)]

    st.dataframe(centroid_df)

    # Váº½ biá»ƒu Ä‘á»“
    fig, ax = plt.subplots(figsize=(6, 6))
    labels = st.session_state.labels
    centroids = st.session_state.centroids

    for i in range(cluster_kmeans):
        ax.scatter(X[labels == i][:, 0], X[labels == i][:, 1], label=f"Cá»¥m {i}", alpha=0.6, edgecolors="k")

    ax.scatter(centroids[:, 0], centroids[:, 1], s=200, c="red", marker="X", label="TÃ¢m cá»¥m")
    ax.set_title(f"K-Means Clustering")
    ax.legend()

    st.pyplot(fig)


from sklearn.datasets import make_moons, make_blobs
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_blobs, make_moons, make_circles
def ly_thuyet_DBSCAN():

    st.markdown(r"""
    ## ğŸ“Œ **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**
    **DBSCAN** lÃ  má»™t thuáº­t toÃ¡n phÃ¢n cá»¥m **khÃ´ng giÃ¡m sÃ¡t**, dá»±a trÃªn **máº­t Ä‘á»™ Ä‘iá»ƒm dá»¯ liá»‡u**, giÃºp xÃ¡c Ä‘á»‹nh cÃ¡c cá»¥m cÃ³ hÃ¬nh dáº¡ng báº¥t ká»³ vÃ  phÃ¡t hiá»‡n nhiá»…u (outliers).  

    ---

    ### ğŸ”¹ **Ã tÆ°á»Ÿng chÃ­nh cá»§a DBSCAN**
    1ï¸âƒ£ **XÃ¡c Ä‘á»‹nh cÃ¡c Ä‘iá»ƒm lÃµi (Core Points):** Náº¿u má»™t Ä‘iá»ƒm cÃ³ Ã­t nháº¥t **min_samples** Ä‘iá»ƒm lÃ¢n cáº­n trong bÃ¡n kÃ­nh **$$ \varepsilon $$**, nÃ³ lÃ  má»™t **Ä‘iá»ƒm lÃµi**.  
    2ï¸âƒ£ **XÃ¡c Ä‘á»‹nh cÃ¡c Ä‘iá»ƒm biÃªn (Border Points):** LÃ  cÃ¡c Ä‘iá»ƒm thuá»™c vÃ¹ng lÃ¢n cáº­n cá»§a Ä‘iá»ƒm lÃµi nhÆ°ng khÃ´ng Ä‘á»§ **min_samples**.  
    3ï¸âƒ£ **XÃ¡c Ä‘á»‹nh nhiá»…u (Noise Points):** CÃ¡c Ä‘iá»ƒm khÃ´ng thuá»™c báº¥t ká»³ cá»¥m nÃ o.  
    4ï¸âƒ£ **Má»Ÿ rá»™ng cá»¥m:** Báº¯t Ä‘áº§u tá»« má»™t Ä‘iá»ƒm lÃµi, má»Ÿ rá»™ng cá»¥m báº±ng cÃ¡ch thÃªm cÃ¡c Ä‘iá»ƒm biÃªn lÃ¢n cáº­n cho Ä‘áº¿n khi khÃ´ng cÃ²n Ä‘iá»ƒm nÃ o thoáº£ mÃ£n Ä‘iá»u kiá»‡n.  

    ---

    ### ğŸ”¢ **Tham sá»‘ quan trá»ng cá»§a DBSCAN**
    - **$$ \varepsilon $$** (eps): BÃ¡n kÃ­nh tÃ¬m kiáº¿m Ä‘iá»ƒm lÃ¢n cáº­n.  
    - **min_samples**: Sá»‘ lÆ°á»£ng Ä‘iá»ƒm tá»‘i thiá»ƒu trong **eps** Ä‘á»ƒ xÃ¡c Ä‘á»‹nh má»™t **core point**.  

    ---

    ### ğŸ“Œ **CÃ´ng thá»©c khoáº£ng cÃ¡ch trong DBSCAN**
    DBSCAN sá»­ dá»¥ng **khoáº£ng cÃ¡ch Euclidean** Ä‘á»ƒ xÃ¡c Ä‘á»‹nh **Ä‘iá»ƒm lÃ¢n cáº­n**, Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c:
    """)

    st.latex(r"d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}")

    st.markdown(r"""
    Trong Ä‘Ã³:  
    - $$ d(p, q) $$ lÃ  khoáº£ng cÃ¡ch giá»¯a hai Ä‘iá»ƒm dá»¯ liá»‡u $$ p $$ vÃ  $$ q $$.  
    - $$ p_i $$ vÃ  $$ q_i $$ lÃ  tá»a Ä‘á»™ cá»§a Ä‘iá»ƒm $$ p $$ vÃ  $$ q $$ trong khÃ´ng gian **n** chiá»u.  

    ---

    ### ğŸ”¢ **CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a DBSCAN**
    **Gá»i** táº­p há»£p cÃ¡c Ä‘iá»ƒm náº±m trong bÃ¡n kÃ­nh **$$ \varepsilon $$** cá»§a **$$ p $$** lÃ :
    """)

    st.latex(r"N_{\varepsilon}(p) = \{ q \in D \mid d(p, q) \leq \varepsilon \}")

    st.markdown(r"""
    - Náº¿u $$ |N_{\varepsilon}(p)| \geq $$ min_samples, thÃ¬ **$$ p $$** lÃ  má»™t **core point**.  
    - Náº¿u **$$ p $$** lÃ  **core point**, táº¥t cáº£ cÃ¡c Ä‘iá»ƒm trong $$ N_{\varepsilon}(p) $$ sáº½ Ä‘Æ°á»£c gÃ¡n vÃ o cÃ¹ng má»™t cá»¥m.  
    - Náº¿u má»™t Ä‘iá»ƒm khÃ´ng thuá»™c cá»¥m nÃ o, nÃ³ Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u lÃ  **nhiá»…u**.  

    ---

    ### âœ… **Æ¯u Ä‘iá»ƒm & âŒ NhÆ°á»£c Ä‘iá»ƒm**
    âœ… **Æ¯u Ä‘iá»ƒm:**  
    - Tá»± Ä‘á»™ng tÃ¬m sá»‘ cá»¥m mÃ  **khÃ´ng cáº§n xÃ¡c Ä‘á»‹nh trÆ°á»›c $$ K $$** nhÆ° K-Means.  
    - Xá»­ lÃ½ tá»‘t **cÃ¡c cá»¥m cÃ³ hÃ¬nh dáº¡ng phá»©c táº¡p**.  
    - PhÃ¡t hiá»‡n **outlier** má»™t cÃ¡ch tá»± nhiÃªn.  

    âŒ **NhÆ°á»£c Ä‘iá»ƒm:**  
    - Nháº¡y cáº£m vá»›i **tham sá»‘ $$ \varepsilon $$ vÃ  min_samples**.  
    - KhÃ´ng hoáº¡t Ä‘á»™ng tá»‘t trÃªn **dá»¯ liá»‡u cÃ³ máº­t Ä‘á»™ thay Ä‘á»•i**.  

    ---

    ### ğŸ“Œ **á»¨ng dá»¥ng cá»§a DBSCAN**
    - **PhÃ¡t hiá»‡n gian láº­n tÃ i chÃ­nh**.  
    - **PhÃ¢n tÃ­ch dá»¯ liá»‡u khÃ´ng gian (GIS, báº£n Ä‘á»“)**.  
    - **PhÃ¡t hiá»‡n báº¥t thÆ°á»ng (Anomaly Detection)**.  
    """)


    # Tiáº¿p tá»¥c pháº§n giao diá»‡n cháº¡y DBSCAN


# Táº¡o dá»¯ liá»‡u ngáº«u nhiÃªn
    def generate_data(n_samples, dataset_type):
        if dataset_type == "Cá»¥m Gauss":
            X, _ = make_blobs(n_samples=n_samples, centers=3, cluster_std=1.0, random_state=42)
        elif dataset_type == "Hai vÃ²ng trÄƒng":
            X, _ = make_moons(n_samples=n_samples, noise=0.1, random_state=42)
        else:  # Hai hÃ¬nh trÃ²n lá»“ng nhau
            X, _ = make_circles(n_samples=n_samples, noise=0.05, factor=0.5, random_state=42)
        return X

    # HÃ m cháº¡y DBSCAN
    def run_dbscan(X, eps, min_samples):
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X)
        return labels

    # Giao diá»‡n Streamlit
    st.title("ğŸ” Minh há»a thuáº­t toÃ¡n DBSCAN")

    # TÃ¹y chá»n loáº¡i dá»¯ liá»‡u
    dataset_type = st.radio(
        "Chá»n kiá»ƒu dá»¯ liá»‡u", 
        ["Cá»¥m Gauss", "Hai vÃ²ng trÄƒng", "Hai hÃ¬nh trÃ²n lá»“ng nhau"], 
        key="dataset_type_dbscan"
    )

    num_samples_dbscan = st.slider("Sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u", 50, 500, 200, step=10, key="num_samples_dbscan")
    eps_dbscan = st.slider("BÃ¡n kÃ­nh cá»¥m (eps)", 0.1, 2.0, 0.1, step=0.1, key="eps_dbscan")
    min_samples_dbscan = st.slider("Sá»‘ Ä‘iá»ƒm tá»‘i thiá»ƒu Ä‘á»ƒ táº¡o cá»¥m", 2, 20, 5, key="min_samples_dbscan")

    # Kiá»ƒm tra vÃ  cáº­p nháº­t dá»¯ liá»‡u DBSCAN trong session_state
    if "X_dbscan" not in st.session_state or st.session_state.get("prev_dataset_type") != dataset_type:
        st.session_state.X_dbscan = generate_data(num_samples_dbscan, dataset_type)
        st.session_state.labels_dbscan = np.full(num_samples_dbscan, -1)
        st.session_state.prev_dataset_type = dataset_type 
        
        
        
    X_dbscan = st.session_state.X_dbscan

    # NÃºt Reset Ä‘á»ƒ táº¡o láº¡i dá»¯ liá»‡u
    if st.button("ğŸ”„ Reset", key="reset_dbscan"):
        st.session_state.X_dbscan = generate_data(num_samples_dbscan, dataset_type)
        st.session_state.labels_dbscan = np.full(num_samples_dbscan, -1)

    # NÃºt cháº¡y DBSCAN
    if st.button("â¡ï¸ Cháº¡y DBSCAN"):
        st.session_state.labels_dbscan = run_dbscan(X_dbscan, eps_dbscan, min_samples_dbscan)

    # Váº½ biá»ƒu Ä‘á»“
    fig, ax = plt.subplots(figsize=(6, 6))
    labels = st.session_state.labels_dbscan
    unique_labels = set(labels)

    # MÃ u cho cÃ¡c cá»¥m
    colors = plt.cm.get_cmap("tab10", len(unique_labels))

    for label in unique_labels:
        mask = labels == label
        color = "black" if label == -1 else colors(label)
        ax.scatter(X_dbscan[mask, 0], X_dbscan[mask, 1], color=color, label=f"Cá»¥m {label}" if label != -1 else "Nhiá»…u", edgecolors="k", alpha=0.7)

    ax.set_title(f"Káº¿t quáº£ DBSCAN (eps={eps_dbscan}, min_samples={min_samples_dbscan})")
    ax.legend()

    # Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“
   
    st.pyplot(fig)






# HÃ m váº½ biá»ƒu Ä‘á»“
import streamlit as st
import numpy as np
from tensorflow.keras.datasets import mnist
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from scipy.stats import mode  

import streamlit as st
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from scipy.stats import mode

def split_data():
    st.title("ğŸ“Œ Chia dá»¯ liá»‡u Train/Test")

    # Äá»c dá»¯ liá»‡u
    Xmt = np.load("buoi4/X.npy")
    ymt = np.load("buoi4/y.npy")
    X = Xmt.reshape(Xmt.shape[0], -1)  # Giá»¯ nguyÃªn Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u
    y = ymt.reshape(-1)  

    total_samples = X.shape[0]

    # Thanh kÃ©o chá»n sá»‘ lÆ°á»£ng áº£nh Ä‘á»ƒ train
    num_samples = st.slider("Chá»n sá»‘ lÆ°á»£ng áº£nh Ä‘á»ƒ train:", min_value=1000, max_value=total_samples, value=10000)

    # Thanh kÃ©o chá»n tá»· lá»‡ Train/Test
    test_size = st.slider("Chá»n tá»· lá»‡ test:", min_value=0.1, max_value=0.5, value=0.2)

    if st.button("âœ… XÃ¡c nháº­n & LÆ°u"):
        # Chá»n sá»‘ lÆ°á»£ng áº£nh mong muá»‘n
        X_selected, y_selected = X[:num_samples], y[:num_samples]

        # Chia train/test theo tá»· lá»‡ Ä‘Ã£ chá»n
        X_train, X_test, y_train, y_test = train_test_split(X_selected, y_selected, test_size=test_size, random_state=42)

        # LÆ°u vÃ o session_state Ä‘á»ƒ sá»­ dá»¥ng sau
        st.session_state["X_train"] = X_train
        st.session_state["y_train"] = y_train
        st.session_state["X_test"] = X_test
        st.session_state["y_test"] = y_test

        st.success(f"ğŸ”¹ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chia: Train ({len(X_train)}), Test ({len(X_test)})")

    if "X_train" in st.session_state:
        st.write("ğŸ“Œ Dá»¯ liá»‡u train/test Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ sá»­ dá»¥ng!")


import mlflow
import os
import time
import numpy as np
import plotly.express as px
import streamlit as st
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import mlflow
import mlflow.sklearn
import streamlit as st
import numpy as np
import os
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from scipy.stats import mode

def input_mlflow():
    DAGSHUB_MLFLOW_URI = "https://dagshub.com/Phuocbinh2003/Hoc_may_python.mlflow"
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    os.environ["MLFLOW_TRACKING_USERNAME"] = "Phuocbinh2003"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "c1495823c8f9156923b06f15899e989db7e62052"
    mlflow.set_experiment("Clustering")

def train():
    st.header("âš™ï¸ Chá»n mÃ´ hÃ¬nh & Huáº¥n luyá»‡n")

    if "X_train" not in st.session_state:
        st.warning("âš ï¸ Vui lÃ²ng chia dá»¯ liá»‡u trÆ°á»›c khi train!")
        return

    X_train = st.session_state["X_train"]
    y_train = st.session_state["y_train"]

    X_train_norm = X_train / 255.0  # Chuáº©n hÃ³a

    model_choice = st.selectbox("Chá»n mÃ´ hÃ¬nh:", ["K-Means", "DBSCAN"])

    if model_choice == "K-Means":
        st.markdown("ğŸ”¹ **K-Means**")
        n_clusters = st.slider("ğŸ”¢ Chá»n sá»‘ cá»¥m (K):", 2, 20, 10)
        pca = PCA(n_components=2)
        X_train_pca = pca.fit_transform(X_train_norm)
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)

    elif model_choice == "DBSCAN":
        st.markdown("ğŸ› ï¸ **DBSCAN**")
        eps = st.slider("ğŸ“ BÃ¡n kÃ­nh lÃ¢n cáº­n (eps):", 0.1, 10.0, 0.5)
        min_samples = st.slider("ğŸ‘¥ Sá»‘ Ä‘iá»ƒm tá»‘i thiá»ƒu trong cá»¥m:", 2, 20, 5)
        pca = PCA(n_components=2)
        X_train_pca = pca.fit_transform(X_train_norm)
        model = DBSCAN(eps=eps, min_samples=min_samples)

    input_mlflow()
    run_name = st.text_input("ğŸ”¹ Nháº­p tÃªn Run:", "Default_Run")
    st.session_state["run_name"] = run_name if run_name else "default_run"

    if st.button("ğŸš€ Huáº¥n luyá»‡n mÃ´ hÃ¬nh"):
        with mlflow.start_run(run_name=st.session_state["run_name"]):
            model.fit(X_train_pca)
            st.success("âœ… Huáº¥n luyá»‡n thÃ nh cÃ´ng!")

            labels = model.labels_

            if model_choice == "K-Means":
                label_mapping = {}
                for i in range(n_clusters):
                    mask = labels == i
                    if np.sum(mask) > 0:
                        most_common_label = mode(y_train[mask], keepdims=True).mode[0]
                        label_mapping[i] = most_common_label

                predicted_labels = np.array([label_mapping[label] for label in labels])
                accuracy = np.mean(predicted_labels == y_train)
                st.write(f"ğŸ¯ **Äá»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh:** `{accuracy * 100:.2f}%`")

                # Log vÃ o MLflow
                mlflow.log_param("model", "K-Means")
                mlflow.log_param("n_clusters", n_clusters)
                mlflow.log_metric("accuracy", accuracy)
                mlflow.sklearn.log_model(model, "kmeans_model")

            elif model_choice == "DBSCAN":
                unique_clusters = set(labels) - {-1}
                n_clusters_found = len(unique_clusters)
                noise_ratio = np.sum(labels == -1) / len(labels)
                st.write(f"ğŸ” **Sá»‘ cá»¥m tÃ¬m tháº¥y:** `{n_clusters_found}`")
                st.write(f"ğŸš¨ **Tá»‰ lá»‡ nhiá»…u:** `{noise_ratio * 100:.2f}%`")

                # Log vÃ o MLflow
                mlflow.log_param("model", "DBSCAN")
                mlflow.log_param("eps", eps)
                mlflow.log_param("min_samples", min_samples)
                mlflow.log_metric("n_clusters_found", n_clusters_found)
                mlflow.log_metric("noise_ratio", noise_ratio)
                mlflow.sklearn.log_model(model, "dbscan_model")

            if "models" not in st.session_state:
                st.session_state["models"] = []

            model_name = model_choice.lower().replace(" ", "_")
            count = 1
            new_model_name = model_name
            while any(m["name"] == new_model_name for m in st.session_state["models"]):
                new_model_name = f"{model_name}_{count}"
                count += 1

            st.session_state["models"].append({"name": new_model_name, "model": model})
            st.write(f"ğŸ”¹ **MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vá»›i tÃªn:** `{new_model_name}`")
            st.write(f"ğŸ“‹ **Danh sÃ¡ch cÃ¡c mÃ´ hÃ¬nh:** {[m['name'] for m in st.session_state['models']]}")
            mlflow.end_run()
            st.success(f"âœ… ÄÃ£ log dá»¯ liá»‡u cho **Train_{st.session_state['run_name']}**!")
            st.markdown(f"### ğŸ”— [Truy cáº­p MLflow DAGsHub]({st.session_state['mlflow_url']})")




import streamlit as st
import numpy as np
import random
from sklearn.cluster import KMeans, DBSCAN
import matplotlib.pyplot as plt
from streamlit_drawable_canvas import st_canvas
from PIL import Image, ImageOps
from sklearn.decomposition import PCA

def preprocess_canvas_image(canvas_result):
    if canvas_result.image_data is not None:
        img = Image.fromarray(canvas_result.image_data[:, :, 0].astype(np.uint8))
        img = img.resize((28, 28)).convert("L")  # Resize vÃ  chuyá»ƒn thÃ nh grayscale
        img = np.array(img, dtype=np.float32) / 255.0  # Chuáº©n hÃ³a vá» [0, 1]
        return img.reshape(1, -1)  # Chuyá»ƒn thÃ nh vector 1D
    return None


def du_doan():
    st.header("âœï¸ Váº½ dá»¯ liá»‡u Ä‘á»ƒ dá»± Ä‘oÃ¡n cá»¥m")

    # Kiá»ƒm tra danh sÃ¡ch mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
    if "models" not in st.session_state or not st.session_state["models"]:
        st.warning("âš ï¸ KhÃ´ng cÃ³ mÃ´ hÃ¬nh nÃ o Ä‘Æ°á»£c lÆ°u! HÃ£y huáº¥n luyá»‡n trÆ°á»›c.")
        return

    # Láº¥y danh sÃ¡ch mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u
    model_names = [model["name"] for model in st.session_state["models"]]

    # ğŸ“Œ Chá»n mÃ´ hÃ¬nh
    model_option = st.selectbox("ğŸ” Chá»n mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n:", model_names)
    model = next(m["model"] for m in st.session_state["models"] if m["name"] == model_option)

    # ğŸ†• Cáº­p nháº­t key cho canvas khi nháº¥n "Táº£i láº¡i"
    if "key_value" not in st.session_state:
        st.session_state.key_value = str(random.randint(0, 1000000))

    if st.button("ğŸ”„ Táº£i láº¡i"):
        st.session_state.key_value = str(random.randint(0, 1000000))
        st.rerun()

    # âœï¸ Váº½ dá»¯ liá»‡u
    canvas_result = st_canvas(
        fill_color="black",
        stroke_width=10,
        stroke_color="white",
        background_color="black",
        height=150,
        width=150,
        drawing_mode="freedraw",
        key=st.session_state.key_value,
        update_streamlit=True
    )

    if st.button("Dá»± Ä‘oÃ¡n cá»¥m"):
        img = preprocess_canvas_image(canvas_result)

        if img is not None:
            X_train = st.session_state["X_train"]
            # Hiá»ƒn thá»‹ áº£nh sau xá»­ lÃ½
            st.image(Image.fromarray((img.reshape(28, 28) * 255).astype(np.uint8)), caption="áº¢nh sau xá»­ lÃ½", width=100)

            pca = PCA(n_components=2)
            pca.fit(X_train)
            img_reduced = pca.transform(img.squeeze().reshape(1, -1))  # Sá»­a lá»—i

            # Dá»± Ä‘oÃ¡n vá»›i K-Means hoáº·c DBSCAN
            if isinstance(model, KMeans):
                predicted_cluster = model.predict(img_reduced)[0]  # Dá»± Ä‘oÃ¡n tá»« áº£nh Ä‘Ã£ PCA
                
                # TÃ­nh confidence: khoáº£ng cÃ¡ch Ä‘áº¿n centroid gáº§n nháº¥t
                distances = model.transform(img_reduced)[0]  
                confidence = 1 / (1 + distances[predicted_cluster])  # Äáº£o ngÆ°á»£c khoáº£ng cÃ¡ch thÃ nh Ä‘á»™ tin cáº­y
                
                st.subheader(f"ğŸ”¢ Cá»¥m dá»± Ä‘oÃ¡n: {predicted_cluster}")
                st.write(f"âœ… **Äá»™ tin cáº­y:** {confidence:.2f}")

            elif isinstance(model, DBSCAN):
                model.fit(X_train)  # Fit trÆ°á»›c vá»›i táº­p huáº¥n luyá»‡n
                predicted_cluster = model.fit_predict(img_reduced)[0]

                if predicted_cluster == -1:
                    st.subheader("âš ï¸ Äiá»ƒm nÃ y khÃ´ng thuá»™c cá»¥m nÃ o!")
                else:
                    # TÃ­nh Ä‘á»™ tin cáº­y vá»›i DBSCAN dá»±a trÃªn sá»‘ lÆ°á»£ng Ä‘iá»ƒm lÃ¢n cáº­n
                    core_samples = model.core_sample_indices_
                    confidence = len(core_samples) / len(X_train)  # Tá»· lá»‡ Ä‘iá»ƒm cá»‘t lÃµi trong táº­p huáº¥n luyá»‡n
                    
                    st.subheader(f"ğŸ”¢ Cá»¥m dá»± Ä‘oÃ¡n: {predicted_cluster}")
                    st.write(f"âœ… **Äá»™ tin cáº­y:** {confidence:.2f}")

        else:
            st.error("âš ï¸ HÃ£y váº½ má»™t sá»‘ trÆ°á»›c khi báº¥m Dá»± Ä‘oÃ¡n!")



from datetime import datetime    
import streamlit as st
import mlflow
from datetime import datetime

def show_experiment_selector():
    st.title("ğŸ“Š MLflow")
    
    
    mlflow.set_tracking_uri("https://dagshub.com/Phuocbinh2003/Hoc_may_python.mlflow")
    
    # Láº¥y danh sÃ¡ch táº¥t cáº£ experiments
    experiment_name = "Clustering"
    experiments = mlflow.search_experiments()
    selected_experiment = next((exp for exp in experiments if exp.name == experiment_name), None)

    if not selected_experiment:
        st.error(f"âŒ Experiment '{experiment_name}' khÃ´ng tá»“n táº¡i!")
        return

    st.subheader(f"ğŸ“Œ Experiment: {experiment_name}")
    st.write(f"**Experiment ID:** {selected_experiment.experiment_id}")
    st.write(f"**Tráº¡ng thÃ¡i:** {'Active' if selected_experiment.lifecycle_stage == 'active' else 'Deleted'}")
    st.write(f"**Vá»‹ trÃ­ lÆ°u trá»¯:** {selected_experiment.artifact_location}")

    # Láº¥y danh sÃ¡ch runs trong experiment
    runs = mlflow.search_runs(experiment_ids=[selected_experiment.experiment_id])

    if runs.empty:
        st.warning("âš  KhÃ´ng cÃ³ runs nÃ o trong experiment nÃ y.")
        return

    st.write("### ğŸƒâ€â™‚ï¸ CÃ¡c Runs gáº§n Ä‘Ã¢y:")
    
    # Láº¥y danh sÃ¡ch run_name tá»« params
    run_info = []
    for _, run in runs.iterrows():
        run_id = run["run_id"]
        run_tags = mlflow.get_run(run_id).data.tags
        run_name = run_tags.get("mlflow.runName", f"Run {run_id[:8]}")  # Láº¥y tá»« tags
        run_info.append((run_name, run_id))
    
    # Táº¡o dictionary Ä‘á»ƒ map run_name -> run_id
    run_name_to_id = dict(run_info)
    run_names = list(run_name_to_id.keys())
    
    # Chá»n run theo run_name
    selected_run_name = st.selectbox("ğŸ” Chá»n má»™t run:", run_names)
    selected_run_id = run_name_to_id[selected_run_name]

    # Hiá»ƒn thá»‹ thÃ´ng tin chi tiáº¿t cá»§a run Ä‘Æ°á»£c chá»n
    selected_run = mlflow.get_run(selected_run_id)

    if selected_run:
        st.subheader(f"ğŸ“Œ ThÃ´ng tin Run: {selected_run_name}")
        st.write(f"**Run ID:** {selected_run_id}")
        st.write(f"**Tráº¡ng thÃ¡i:** {selected_run.info.status}")
        
        start_time_ms = selected_run.info.start_time  # Thá»i gian lÆ°u dÆ°á»›i dáº¡ng milliseconds
        if start_time_ms:
            start_time = datetime.fromtimestamp(start_time_ms / 1000).strftime("%Y-%m-%d %H:%M:%S")
        else:
            start_time = "KhÃ´ng cÃ³ thÃ´ng tin"
        
        st.write(f"**Thá»i gian cháº¡y:** {start_time}")

        # Hiá»ƒn thá»‹ thÃ´ng sá»‘ Ä‘Ã£ log
        params = selected_run.data.params
        metrics = selected_run.data.metrics

        if params:
            st.write("### âš™ï¸ Parameters:")
            st.json(params)

        if metrics:
            st.write("### ğŸ“Š Metrics:")
            st.json(metrics)

        # Kiá»ƒm tra loáº¡i mÃ´ hÃ¬nh vÃ  hiá»ƒn thá»‹ thÃ´ng tin tÆ°Æ¡ng á»©ng
        model_type = params.get("model", "Unknown")
        if model_type == "K-Means":
            st.write(f"ğŸ”¹ **MÃ´ hÃ¬nh:** K-Means")
            st.write(f"ğŸ”¢ **Sá»‘ cá»¥m (K):** {params.get('n_clusters', 'N/A')}")
            st.write(f"ğŸ¯ **Äá»™ chÃ­nh xÃ¡c:** {metrics.get('accuracy', 'N/A')}")
        elif model_type == "DBSCAN":
            st.write(f"ğŸ› ï¸ **MÃ´ hÃ¬nh:** DBSCAN")
            st.write(f"ğŸ“ **eps:** {params.get('eps', 'N/A')}")
            st.write(f"ğŸ‘¥ **Min Samples:** {params.get('min_samples', 'N/A')}")
            st.write(f"ğŸ” **Sá»‘ cá»¥m tÃ¬m tháº¥y:** {metrics.get('n_clusters_found', 'N/A')}")
            st.write(f"ğŸš¨ **Tá»‰ lá»‡ nhiá»…u:** {metrics.get('noise_ratio', 'N/A')}")

        # Hiá»ƒn thá»‹ model artifact
        model_artifact_path = f"{selected_experiment.artifact_location}/{selected_run_id}/artifacts/{model_type.lower()}_model"
        st.write("### ğŸ“‚ Model Artifact:")
        st.write(f"ğŸ“¥ [Táº£i mÃ´ hÃ¬nh]({model_artifact_path})")
    else:
        st.warning("âš  KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin cho run nÃ y.")




def ClusteringAlgorithms():
  

    st.title("ğŸ–Šï¸ MNIST Classification App")

    
    
   
    # === Sidebar Ä‘á»ƒ chá»n trang ===
    # === Táº¡o Tabs ===
    tab1, tab2, tab3, tab4,tab5 ,tab6= st.tabs(["ğŸ“˜ LÃ½ thuyáº¿t K-means", "ğŸ“˜ LÃ½ thuyáº¿t DBSCAN", "ğŸ“˜ Data" ,"âš™ï¸ Huáº¥n luyá»‡n", "ğŸ”¢ Dá»± Ä‘oÃ¡n","ğŸ”¥ Mlflow"])

    with tab1:
        ly_thuyet_K_means()

    with tab2:
        ly_thuyet_DBSCAN()
    
    with tab3:
        data()
        
    with tab4:
       
        
        
        
        split_data()
        train()
        
    
    with tab5:
        
        du_doan() 
    with tab6:
        
        show_experiment_selector() 




            
if __name__ == "__main__":
    ClusteringAlgorithms()
